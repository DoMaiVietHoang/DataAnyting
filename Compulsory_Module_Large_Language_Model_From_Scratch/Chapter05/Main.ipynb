{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert(d_out%num_heads==0), \"d_out must devide into d_in\"\n",
    "        self.head_dim = d_out//num_heads\n",
    "        self.W_q      = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k      = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v      = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.dropout  = nn.Dropout(p=dropout)\n",
    "        self.projection = nn.Linear(d_out, d_out)\n",
    "        self.num_heads= num_heads\n",
    "        self.d_out    = d_out\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "        queries = self.W_q(x)\n",
    "        keys    = self.W_k(x)\n",
    "        values  = self.W_v(x)\n",
    "        queries = queries.view(batch_size, num_tokens, self.num_heads,self.head_dim)\n",
    "        keys    = keys   .view(batch_size, num_tokens, self.num_heads,self.head_dim)\n",
    "        values  = values .view(batch_size, num_tokens, self.num_heads,self.head_dim)\n",
    "        queries = queries.transpose(1,2)\n",
    "        keys    = keys.transpose(1,2)\n",
    "        values  = values.transpose(1,2)\n",
    "        attention_scores = queries@keys.transpose(2,3)\n",
    "        mask_bool        = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "        attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attenton_weights = torch.softmax(attention_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        attenton_weights = self.dropout(attenton_weights)\n",
    "        context_vec      = (attenton_weights@values).transpose(1,2)\n",
    "        context_vec      = context_vec.contiguous().view(\n",
    "            batch_size, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec      = self.projection(context_vec)\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.esp = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var  = x.var(dim=-1,keepdim=True)\n",
    "        normalization = (x-mean)/torch.sqrt(var+self.esp)\n",
    "        return self.scale*normalization+self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.Layer = nn.Sequential(\n",
    "         nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),\n",
    "         nn.GELU(),\n",
    "         nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
    "    \n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.Layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransFormerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.LayerNorm   = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.Muitiheads  = MaskedMultiheadAttention(d_in=cfg[\"emb_dim\"], d_out = cfg[\"emb_dim\"], context_length=cfg[\"context_length\"], \n",
    "                                                    dropout=cfg[\"drop_rate\"], num_heads=cfg[\"n_heads\"])\n",
    "        self.FeedForward = FeedForward(cfg)\n",
    "        self.Dropout     = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    def forward(self,x):\n",
    "        shortcut = x\n",
    "        x        = self.LayerNorm(x)\n",
    "        x        = self.Muitiheads(x)\n",
    "        x        = self.Dropout(x)\n",
    "        x        = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x        = self.LayerNorm(x)\n",
    "        x        = self.FeedForward(x)\n",
    "        x        = self.Dropout(x)\n",
    "        x        = x+ shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    " \"vocab_size\": 50257, # Vocabulary size\n",
    " \"context_length\": 1024, # Context length\n",
    " \"emb_dim\": 768, # Embedding dimension\n",
    " \"n_heads\": 12, # Number of attention heads\n",
    " \"n_layers\": 12, # Number of layers\n",
    " \"drop_rate\": 0.1, # Dropout rate\n",
    " \"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"]    , cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout (cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransFormerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.LayerNorm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head  = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"])\n",
    "        #self.dropout   = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    def forward(self,x):\n",
    "        batch_size, sequen = x.shape\n",
    "        tok_emb            = self.tok_emb(x)\n",
    "        pos_emb            = self.pos_emb(torch.arange(sequen))\n",
    "        input              = tok_emb + pos_emb\n",
    "        input              = self.drop_emb(input)\n",
    "        input              = self.trf_blocks(input)\n",
    "        output             = self.LayerNorm(input)\n",
    "        output             = self.out_head(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 1745,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day hold a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransFormerBlock(\n",
       "      (LayerNorm): LayerNorm()\n",
       "      (Muitiheads): MaskedMultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (Layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransFormerBlock(\n",
       "      (LayerNorm): LayerNorm()\n",
       "      (Muitiheads): MaskedMultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (Layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransFormerBlock(\n",
       "      (LayerNorm): LayerNorm()\n",
       "      (Muitiheads): MaskedMultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (Layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransFormerBlock(\n",
       "      (LayerNorm): LayerNorm()\n",
       "      (Muitiheads): MaskedMultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (Layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransFormerBlock(\n",
       "      (LayerNorm): LayerNorm()\n",
       "      (Muitiheads): MaskedMultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (Layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransFormerBlock(\n",
       "      (LayerNorm): LayerNorm()\n",
       "      (Muitiheads): MaskedMultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (Layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransFormerBlock(\n",
       "      (LayerNorm): LayerNorm()\n",
       "      (Muitiheads): MaskedMultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (Layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransFormerBlock(\n",
       "      (LayerNorm): LayerNorm()\n",
       "      (Muitiheads): MaskedMultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (Layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransFormerBlock(\n",
       "      (LayerNorm): LayerNorm()\n",
       "      (Muitiheads): MaskedMultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (Layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransFormerBlock(\n",
       "      (LayerNorm): LayerNorm()\n",
       "      (Muitiheads): MaskedMultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (Layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransFormerBlock(\n",
       "      (LayerNorm): LayerNorm()\n",
       "      (Muitiheads): MaskedMultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (Layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransFormerBlock(\n",
       "      (LayerNorm): LayerNorm()\n",
       "      (Muitiheads): MaskedMultiheadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (FeedForward): FeedForward(\n",
       "        (Layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (LayerNorm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probas = torch.softmax (logits , dim=-1)\n",
    "            idx_next = torch.argmax(probas , dim=-1, keepdim=True)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Aeiman ByeswickattributeometerSin 19elve Chal\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "def tokenid_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    " model=model,\n",
    " idx=text_to_token_ids(start_context, tokenizer),\n",
    " max_new_tokens=10,\n",
    " context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", tokenid_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
