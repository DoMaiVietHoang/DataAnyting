{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert(d_out%num_heads==0), \"d_out must devide into d_in\"\n",
    "        self.head_dim = d_out//num_heads\n",
    "        self.W_q      = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k      = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v      = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.dropout  = nn.Dropout(p=dropout)\n",
    "        self.projection = nn.Linear(d_out, d_out)\n",
    "        self.num_heads= num_heads\n",
    "        self.d_out    = d_out\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "        queries = self.W_q(x)\n",
    "        keys    = self.W_k(x)\n",
    "        values  = self.W_V(x)\n",
    "        queries = queries.view(batch_size, num_tokens, self.num_heads,self.head_dim)\n",
    "        keys    = keys.view(batch_size, num_tokens, self.num_heads,self.head_dim)\n",
    "        values  = values.view(batch_size, num_tokens, self.num_heads,self.head_dim)\n",
    "        queries.transpose(1,2)\n",
    "        keys.transpose(1,2)\n",
    "        values.transpose(1,2)\n",
    "        attention_scores = queries@keys.transpose(2,3)\n",
    "        mask_bool        = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "        attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attenton_weights = torch.softmax(attention_scores/keys.shape[-1]**0.5)\n",
    "        attenton_weights = self.dropout(attenton_weights)\n",
    "        context_vec      = (attenton_weights@values).transpose(1,2)\n",
    "        context_vec      = context_vec.contiguous().view(\n",
    "            batch_size, num_tokens, self.d_out\n",
    "        )\n",
    "        context_vec      = self.projection(context_vec)\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.esp = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var  = x.var(dim=-1,keepdim=True)\n",
    "        normalization = (x-mean)/torch.sqrt(var+self.esp)\n",
    "        return self.scale*normalization+self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__()\n",
    "        self.Layer = nn.Sequential(\n",
    "         nn.Linear(cfg[\"emb_size\"],4*cfg[\"emb_size\"]),\n",
    "         nn.GELU(),\n",
    "         nn.Linear(4*cfg[\"emb_size\"],cfg[\"emb_size\"])\n",
    "    \n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.Layer(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
